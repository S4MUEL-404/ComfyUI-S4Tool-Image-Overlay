# è¾…åŠ©å‡½æ•°å®šä¹‰ï¼ˆå…±äº«ç»™å…¶ä»–æ–‡ä»¶ä½¿ç”¨ï¼‰
def pil2tensor(image):
    from PIL import Image
    import torch
    import numpy as np
    # ç¡®ä¿å›¾åƒæ˜¯ RGB æˆ– RGBA æ ¼å¼
    if image.mode not in ['RGB', 'RGBA']:
        image = image.convert('RGBA')
    array = np.array(image).astype(np.float32) / 255.0
    print(f"pil2tensor array shape: {array.shape}, min: {array.min()}, max: {array.max()}")  # è°ƒè¯•
    return torch.from_numpy(array).unsqueeze(0)

def tensor2pil(image):
    from PIL import Image
    import torch
    import numpy as np
    
    # ç¡®ä¿æ˜¯ PyTorch å¼ é‡
    if not isinstance(image, torch.Tensor):
        raise ValueError(f"Input must be a PyTorch tensor, got {type(image)} with shape {image.shape}")
    
    # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if image.dim() == 4:  # (batch, channels, height, width)
        if image.size(0) > 1:
            print(f"Warning: Multiple batches detected, using first batch. Shape: {image.shape}")
        image = image[0]  # æå–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    elif image.dim() == 3:  # (channels, height, width) æˆ– (height, width, channels)
        image = image
    else:
        raise ValueError(f"Unexpected tensor dimension: {image.dim()} with shape {image.shape}")

    # è½¬æ¢ä¸º numpy æ•°ç»„ï¼Œå¹¶ç¼©æ”¾åˆ° 0-255
    array = image.cpu().numpy()
    print(f"tensor2pil array shape before transpose: {array.shape}, min: {array.min()}, max: {array.max()}")  # è°ƒè¯•

    # è°ƒæ•´ç»´åº¦é¡ºåºï¼Œç¡®ä¿ (channels, height, width)
    if array.ndim == 3 and array.shape[0] not in [1, 3, 4]:  # (height, width, channels)
        array = np.transpose(array, (2, 0, 1))  # è½¬æ¢ä¸º (channels, height, width)
    elif array.ndim == 2:  # (height, width)ï¼Œå¯èƒ½æ˜¯å•é€šé“
        array = array[np.newaxis, :, :]  # è½¬æ¢ä¸º (1, height, width)
    elif array.ndim != 3:
        raise ValueError(f"Unexpected array shape: {array.shape}")

    # éªŒè¯é€šé“æ•°
    channels = array.shape[0]
    if channels not in [1, 3, 4]:
        raise ValueError(f"Unexpected number of channels: {channels} with shape {array.shape}")
    
    # ç¼©æ”¾åˆ° 0-255 èŒƒå›´
    array = np.clip(array * 255.0, 0, 255)
    print(f"tensor2pil array shape after transpose: {array.shape}, min: {array.min()}, max: {array.max()}")  # è°ƒè¯•

    # è½¬æ¢ä¸º PIL å›¾åƒ
    if channels == 1:  # å•é€šé“ï¼ˆå¦‚ MASKï¼‰
        return Image.fromarray(array[0].astype(np.uint8), mode='L')
    elif channels == 3:  # RGB
        return Image.fromarray(np.transpose(array, (1, 2, 0)).astype(np.uint8), mode='RGB')
    elif channels == 4:  # RGBA
        return Image.fromarray(np.transpose(array, (1, 2, 0)).astype(np.uint8), mode='RGBA')
    else:
        raise ValueError(f"Unexpected number of channels: {channels} with shape {array.shape}")

# ImageOverlay ç±»
class ImageOverlay:
    """
    ä¸€ä¸ªå°†ä¸¤å¼ å›¾ç‰‡è¿›è¡Œæ··åˆçš„èŠ‚ç‚¹ï¼Œé€šè¿‡åœ¨æŒ‡å®š X,Y åæ ‡å¤„å°† Layer image æ”¾ç½®åœ¨ Background image ä¸Šï¼Œ
    æ”¯æŒé•œåƒã€æ—‹è½¬ã€ç¼©æ”¾å’Œ alpha æ··åˆä»¥å¤„ç†é€æ˜åº¦ã€‚
    """
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "Layer image": ("IMAGE",),  # å‰æ™¯å›¾ç‰‡
                "Background image": ("IMAGE",),  # èƒŒæ™¯å›¾ç‰‡
                "x_position": ("INT", {
                    "default": 0,
                    "min": -4096,
                    "max": 4096,
                    "step": 1,
                    "display": "number"
                }),
                "y_position": ("INT", {
                    "default": 0,
                    "min": -4096,
                    "max": 4096,
                    "step": 1,
                    "display": "number"
                }),
                "mirror": (["None", "Horizontal", "Vertical"], {
                    "default": "None"
                }),
                "rotation": ("FLOAT", {
                    "default": 0.0,
                    "min": -360.0,
                    "max": 360.0,
                    "step": 1.0,
                    "display": "number"
                }),
                "scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.01,
                    "max": 100.0,
                    "step": 0.01,
                    "display": "number"
                }),
            },
            "optional": {
                "Layer mask (optional)": ("MASK",),  # å¯é€‰çš„æ©ç è¾“å…¥
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("blended_image",)
    FUNCTION = "blend_images"
    CATEGORY = "ğŸ’€S4Tool"
    OUTPUT_NODE = False

    def blend_images(self, **kwargs):
        import torch
        import numpy as np
        from PIL import Image

        # æå–å‚æ•°
        Layer_image = kwargs.get("Layer image")
        Background_image = kwargs.get("Background image")
        x_position = kwargs.get("x_position", 0)
        y_position = kwargs.get("y_position", 0)
        mirror = kwargs.get("mirror", "None")
        rotation = kwargs.get("rotation", 0.0)
        scale = kwargs.get("scale", 1.0)
        Layer_mask = kwargs.get("Layer mask (optional)")

        # è°ƒè¯•ï¼šæ‰“å°åŸå§‹å¼ é‡å½¢çŠ¶
        print(f"Layer image tensor shape: {Layer_image.shape}")
        print(f"Background image tensor shape: {Background_image.shape}")

        # è½¬æ¢ä¸º PIL å›¾åƒï¼Œç¡®ä¿ RGBA æ¨¡å¼ä»¥æ”¯æŒé€æ˜åº¦
        background_pil = tensor2pil(Background_image).convert('RGBA')
        layer_pil = tensor2pil(Layer_image).convert('RGBA')  # å¼ºåˆ¶è½¬æ¢ä¸º RGBA æ¨¡å¼

        # è°ƒè¯•ï¼šæ£€æŸ¥ layer_pil çš„åƒç´ å€¼
        layer_array = np.array(layer_pil)
        print(f"Layer PIL pixel values: shape: {layer_array.shape}, min: {layer_array.min()/255}, max: {layer_array.max()/255}")

        # æå– Layer image çš„ alpha é€šé“ä½œä¸ºé»˜è®¤æ©ç 
        layer_alpha = layer_pil.split()[-1]
        print(f"Layer alpha size: {layer_alpha.size}, mode: {layer_alpha.mode}, min: {np.array(layer_alpha).min()/255}, max: {np.array(layer_alpha).max()/255}")  # è°ƒè¯•

        # å¤„ç†å¯é€‰æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
        if Layer_mask is not None:
            mask_pil = tensor2pil(Layer_mask).convert('L')
            mask_array = np.array(mask_pil)
            print(f"Layer mask tensor shape: {Layer_mask.shape}")
            print(f"Layer mask PIL size: {mask_pil.size}, mode: {mask_pil.mode}, min: {mask_array.min()/255}, max: {mask_array.max()/255}")
            if mask_pil.size != layer_pil.size:
                print(f"Resizing mask from {mask_pil.size} to {layer_pil.size}")
                mask_pil = mask_pil.resize(layer_pil.size, Image.Resampling.LANCZOS)
            # ç¡®ä¿æ©ç é€»è¾‘æ­£ç¡®ï¼ˆé»‘è‰²ä¸ºé€æ˜ï¼Œç™½è‰²ä¸ºä¸é€æ˜ï¼‰
            if mask_array.max() <= 1.0:  # å‡è®¾ mask å·²å½’ä¸€åŒ–
                mask_array = mask_array * 255
            if mask_array.max() == 255 and mask_array.min() == 0:
                layer_alpha = Image.fromarray(255 - mask_array.astype(np.uint8), mode='L')  # åè½¬æ©ç 
            else:
                layer_alpha = Image.fromarray(mask_array.astype(np.uint8), mode='L')

        # åº”ç”¨ç¼©æ”¾
        target_width = int(layer_pil.width * scale)
        target_height = int(layer_pil.height * scale)
        if target_width != layer_pil.width or target_height != layer_pil.height:
            print(f"Scaling layer from {layer_pil.size} to ({target_width}, {target_height})")
            layer_pil = layer_pil.resize((target_width, target_height), Image.Resampling.LANCZOS)
            layer_alpha = layer_alpha.resize((target_width, target_height), Image.Resampling.LANCZOS)

        # åº”ç”¨é•œåƒ
        if mirror == "Horizontal":
            layer_pil = layer_pil.transpose(Image.FLIP_LEFT_RIGHT)
            layer_alpha = layer_alpha.transpose(Image.FLIP_LEFT_RIGHT)
        elif mirror == "Vertical":
            layer_pil = layer_pil.transpose(Image.FLIP_TOP_BOTTOM)
            layer_alpha = layer_alpha.transpose(Image.FLIP_TOP_BOTTOM)

        # åº”ç”¨æ—‹è½¬
        if rotation != 0:
            layer_pil = layer_pil.rotate(rotation, expand=True)
            layer_alpha = layer_alpha.rotate(rotation, expand=True)

        # è®¡ç®—ç²˜è´´ä½ç½®ï¼ŒåŸºäº Layer image çš„è°ƒæ•´åå¤§å°
        x = x_position
        y = y_position
        original_height = Layer_image.shape[2]  # åŸå§‹é«˜åº¦
        original_width = Layer_image.shape[3]   # åŸå§‹å®½åº¦
        target_height = int(original_height * scale)  # è°ƒæ•´åé«˜åº¦
        target_width = int(original_width * scale)    # è°ƒæ•´åå®½åº¦
        paste_box = (x, y, x + target_width, y + target_height)
        print(f"Initial Paste box: {paste_box}")  # è°ƒè¯•

        # ç¡®ä¿ç²˜è´´åŒºåŸŸåœ¨èƒŒæ™¯å›¾åƒèŒƒå›´å†…
        paste_box = (
            max(0, x),
            max(0, y),
            min(background_pil.width, x + target_width),
            min(background_pil.height, y + target_height)
        )
        print(f"Adjusted paste box: {paste_box}")  # è°ƒè¯•

        # åˆ›å»ºä¸­é—´ç”»å¸ƒï¼Œå¤„ç†å‰æ™¯å›¾åƒå’Œæ©ç 
        comp = Image.new('RGBA', background_pil.size, (0, 0, 0, 0))  # é€æ˜ç”»å¸ƒ
        comp.paste(layer_pil, (paste_box[0], paste_box[1]), layer_alpha)
        comp_mask = Image.new('L', background_pil.size, 0)  # é»‘è‰²æ©ç 
        comp_mask.paste(layer_alpha, (paste_box[0], paste_box[1]))

        # éªŒè¯ alpha æ©ç æœ‰æ•ˆæ€§
        alpha_array = np.array(comp_mask)
        print(f"Comp mask values: min: {alpha_array.min()/255}, max: {alpha_array.max()/255}")
        if alpha_array.max() == 0:
            print("Warning: Comp mask is fully transparent, forcing default alpha")
            comp_mask = Image.new('L', comp.size, 255)  # å…¨ä¸é€æ˜æ©ç 

        # å°†ä¸­é—´ç”»å¸ƒä¸èƒŒæ™¯æ··åˆ
        background_pil = Image.composite(comp, background_pil, comp_mask)
        print(f"Background size after paste: {background_pil.size}, min: {np.array(background_pil.convert('RGB')).min()/255}, max: {np.array(background_pil.convert('RGB')).max()/255}")  # è°ƒè¯•

        # è½¬æ¢ä¸ºå¼ é‡ï¼Œè¾“å‡º RGB æ ¼å¼
        blended_image = pil2tensor(background_pil.convert('RGB'))
        return (blended_image,)